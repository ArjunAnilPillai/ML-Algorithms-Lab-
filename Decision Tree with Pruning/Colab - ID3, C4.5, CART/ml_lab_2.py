# -*- coding: utf-8 -*-
"""ML_Lab_2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZkSxH5G43ppX86sYiAknBHwRI49YODQ3

#Name - Arjun A.
#Roll number - 181CO109
#Date of submission - 22-01-2021
This notebook was written in google colab. <br>Link to view notebook<br>
https://colab.research.google.com/drive/1ZkSxH5G43ppX86sYiAknBHwRI49YODQ3?usp=sharing

#Machine Learning Lab 2
  Demonstrate working of decision tree based ID3 algorithm, C4.5 algorithm, CART algorithm. Find and compare accuracy of each algorithm. Do tree pruning to prevent over-fitting. Use any dataset as seen fit.

##Importing required packages
"""

!pip install numpy
!pip install scipy
!pip install -U scikit-learn
!pip install pandas
!pip install graphviz

import numpy as np 
import pandas as pd 
import graphviz
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split 
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz
from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report 
from sklearn.datasets import load_breast_cancer

"""##Importing data from the breast cancer dataset
Using the load_breast_cancer function from sklearn, importing the breast cancer dataset

"""

# Function to import data
def importdata(): 

  breastcancerData = load_breast_cancer()
  X = breastcancerData.data
  Y = breastcancerData.target
  names = breastcancerData.target_names
  featureName = breastcancerData.feature_names
  print(type(X))
  print(type(Y))
  print(names)
  
  return X, Y, names, featureName

X, Y, bcClassNames, bcFeatureNames = importdata()

"""##Splitting the data into train and test sets
 Splitting the data in the ratio of 7:3. (70% training and 30% testing)
"""

def splitdataset(X, Y): 

  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 100) #Specifying random_state to get the same dataset split everytime 
	
  return X, Y, X_train, X_test, y_train, y_test

X, Y, X_train, X_test, y_train, y_test = splitdataset(X, Y)

#Use to print the entire dataset
#print(X, Y, X_train, X_test, y_train, y_test, sep = '\n\n')

#Printing size of the split
print('Test dataset size\nX_test -', len(X_test), '\ny_test -', len(y_test), '\n')
print('Train dataset size\nX_train -', len(X_train), '\ny_train -', len(y_train))

"""##Decision tree - ID3 algorithm
Using sklearn's DecisionTreeClassifier to implement ID3 decision tree algorithm.  <br>
The 'max_depth' parameter is set to 10 i.e maximum depth of the tree is set to 10. <br>
The 'splitter' is set to 'best' to ensure that the attribute with the highest information gain is selected everytime.<br>
The 'random_state' attribute is set to 100 to ensure that the same decision tree is obtained each time.  

"""

modelID3 = DecisionTreeClassifier(criterion = 'entropy', splitter = 'best', random_state = 100)
modelID3.fit(X_train, y_train)

"""##Decision tree - C4.5 algorithm
Refer this for program <br>https://github.com/ArjunAnilPillai/scikit-learn-C4.5-tree-classifier.git
"""

#Code to clone package for C4.5 decision tree
!git clone https://github.com/ArjunAnilPillai/scikit-learn-C4.5-tree-classifier.git

# Commented out IPython magic to ensure Python compatibility.
'''#Code to pull some code from my github
# %cd scikit-learn-C4.5-tree-classifier/ 
!ls
!git pull
# %cd ..
!ls'''

import sys
sys.path.append('/content/scikit-learn-C4.5-tree-classifier')

from c45 import C45
modelC45 = C45(bcFeatureNames)
modelC45.fit(X_train, y_train)

"""##Decision tree - CART algorithm
Using sklearn's DecisionTreeClassifier to implement CART decision tree algorithm.  <br>
The 'maxdepth' parameter is set to 10 i.e maximum depth of the tree is set to 10. <br>
The 'splitter' is set to 'best' to ensure that the attribute with the highest gini index is selected everytime.<br>
The 'random_state' attribute is set to 100 to ensure that the same decision tree is obtained each time.  
"""

modelCART = DecisionTreeClassifier(criterion = 'gini',splitter = 'best', random_state = 100)
modelCART.fit(X_train, y_train)

"""##Function to print accuracy 
Using sklearn's classification_report and accuracy_score.
"""

def measuringAccuracy(Y_train, y_test, y_predict_train, y_predict_test):
  print('Accuracy with train data -', (accuracy_score(y_train, y_predict_train) * 100))
  print('Accuracy with test data -', (accuracy_score(y_test, y_predict_test) * 100))
  print('\nReport for test data\n', classification_report(y_test, y_predict_test))

"""##Calculating accuracy and generating report for the different algorithms

###For ID3 decision tree
"""

#ID3 decision tree
print("ID3 decision tree")
y_predID3 = modelID3.predict(X_test)
y_predID3_train = modelID3.predict(X_train)
measuringAccuracy(y_train, y_test, y_predID3_train, y_predID3)
plot_tree(modelID3)

#Graphviz for drawing tree
dot_data = export_graphviz(modelID3, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("ID3_Decision_tree_before_pruning")

"""###For C4.5 decision tree"""

#C45 decision tree
print("C45 decision tree")
y_predictC45 = modelC45.predict(X_test)
y_predictC45_train = modelC45.predict(X_train)
measuringAccuracy(y_train , y_test, y_predictC45_train, y_predictC45)

#Printing XML for the decision tree and copying into .txt file
xmlC45String, xmlC45 = modelC45.printTree()
print(xmlC45String)

# Write xml into file
with open('/content/C45_Decision_tree_before_pruning.txt', 'w') as writefile:
  writefile.write(xmlC45String)

"""###For CART decision tree"""

#CART decision tree
print("CART decision tree")
y_predCART = modelCART.predict(X_test)
y_predCART_train = modelCART.predict(X_train)
measuringAccuracy(y_train, y_test, y_predCART_train, y_predCART)
plot_tree(modelCART)

#Graphviz for drawing tree
dot_data = export_graphviz(modelCART, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("CART_Decision_tree_before_pruning")

"""##Doing post-pruning for the decision trees 
This is done using Cost Complexity Pruning which is provided by sci-kit learn

###For ID3
"""

#ID3 Decision tree classifier
modelID3 = DecisionTreeClassifier(criterion = 'entropy', splitter = 'best', random_state = 100)

#Calculatiing ccp_alphas
path = modelID3.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

#Plotting ccp_alphas vs total impurity of the leaf nodes
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("Effective alpha")
ax.set_ylabel("Total Impurity of leaves")
ax.set_title("Total Impurity vs Effective alpha for training set")

clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(criterion = 'entropy', splitter = 'best', random_state = 100, ccp_alpha = ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

#Printing out the tree with just the root node
print("Number of nodes in the last tree is {} with ccp_alpha = {}".format(clfs[-1].tree_.node_count, ccp_alphas[-1]))

#Getting rid of this trivial tree
clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

#Plotting number of nodes vs alpha and max depth vs alpha
node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas, node_counts, marker = 'o', drawstyle = "steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker = 'o', drawstyle = "steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("Depth of tree")
ax[1].set_title("Max Depth of the tree vs alpha")
fig.tight_layout()

#Finding test and train prediction accuracy scores
train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]

#Plotting accuracy vs alpha for test and train
fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker='o', label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker='o', label="test", drawstyle="steps-post")
ax.legend()
plt.show()

index_best_model = np.argmax(test_scores)
modelID3 = clfs[index_best_model]

"""###For C4.5"""

#Trying to find optimum depth 
clfs = []
clfsTrain = []

clf = C45(bcFeatureNames)
clf.fit_fixed_depth(X_train, y_train, 1)
clfs.append(accuracy_score(y_test, clf.predict(X_test)))
clfsTrain.append(accuracy_score(y_train, clf.predict(X_train)))
modelC45 = clf
cur_acc = clfs[-1]
depth = 1

for i in range(2, 10):
  clf = C45(bcFeatureNames)
  clf.fit_fixed_depth(X_train, y_train, i)
  clfs.append(accuracy_score(y_test, clf.predict(X_test)))
  clfsTrain.append(accuracy_score(y_train, clf.predict(X_train)))
  cur_acc_model = clfs[-1]
  if cur_acc_model > cur_acc:
    cur_acc = cur_acc_model
    modelC45 = clf
    depth = i

x_axis = [i for i in range(1, 10)]

#Plotting graph for max_depth 
fig, ax = plt.subplots()
ax.plot(x_axis, clfs, marker='o', drawstyle="steps-post", label = "test")
ax.plot(x_axis, clfsTrain, marker='o', drawstyle="steps-post", label = "train")
ax.set_xlabel("Effective alpha")
ax.set_ylabel("Total Impurity of leaves")
ax.set_title("Total Impurity vs Effective alpha for training set")

"""###For CART"""

#CART Decision tree classifier
modelCART = DecisionTreeClassifier(criterion = 'gini',splitter = 'best', random_state = 100)

#Calculatiing ccp_alphas
path = modelCART.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

#Plotting ccp_alphas vs total impurity of the leaf nodes
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("Effective alpha")
ax.set_ylabel("Total Impurity of leaves")
ax.set_title("Total Impurity vs Effective alpha for training set")

clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(criterion = 'gini',splitter = 'best', random_state = 100, ccp_alpha = ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

#Printing out the tree with just the root node
print("Number of nodes in the last tree is {} with ccp_alpha = {}".format(clfs[-1].tree_.node_count, ccp_alphas[-1]))

#Getting rid of this trivial tree
clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

#Plotting number of nodes vs alpha and max depth vs alpha
node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas, node_counts, marker = 'o', drawstyle = "steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker = 'o', drawstyle = "steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("Depth of tree")
ax[1].set_title("Max Depth of the tree vs alpha")
fig.tight_layout()

#Finding test and train prediction accuracy scores
train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]

#Plotting accuracy vs alpha for test and train
fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker='o', label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker='o', label="test", drawstyle="steps-post")
ax.legend()
plt.show()

index_best_model = np.argmax(test_scores)
modelCART = clfs[index_best_model]

"""##Calculating accuracy and generating report for the different algorithms after pruning

###For ID3 decision tree
"""

#ID3 decision tree
print("ID3 decision tree")
y_predID3 = modelID3.predict(X_test)
y_predID3_train = modelID3.predict(X_train)
measuringAccuracy(y_train, y_test, y_predID3_train, y_predID3)
plot_tree(modelID3)

#Graphviz for drawing tree
dot_data = export_graphviz(modelID3, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("ID3_Decision_tree_after_pruning")

"""###For C4.5 decision tree"""

#C45 decision tree
print("C45 decision tree")
y_predictC45 = modelC45.predict(X_test)
y_predictC45_train = modelC45.predict(X_train)
measuringAccuracy(y_train , y_test, y_predictC45_train, y_predictC45)


#Printing XML for the decision tree and copying into .txt file
xmlC45String, xmlC45 = modelC45.printTree()
print(xmlC45String)

# Write xml into file
with open('/content/C45_Decision_tree_after_pruning.txt', 'w') as writefile:
  writefile.write(xmlC45String)

"""###For CART decision tree"""

#CART decision tree
print("CART decision tree")
y_predCART = modelCART.predict(X_test)
y_predCART_train = modelCART.predict(X_train)
measuringAccuracy(y_train, y_test, y_predCART_train, y_predCART)
plot_tree(modelCART)

#Graphviz for drawing tree
dot_data = export_graphviz(modelCART, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("CART_Decision_tree_after_pruning")